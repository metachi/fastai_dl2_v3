{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a reimplementation of fastai part 2 version 3 in Swift. https://github.com/fastai/fastai_docs/blob/master/dev_course/dl2/03_minibatch_training.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(url: \"https://github.com/mxcl/Path.swift\", from: \"0.16.1\")\n",
      "\t\tPath\n",
      "With SwiftPM flags: []\n",
      "Working in: /tmp/tmp4wyjq4xd\n",
      "Fetching https://github.com/mxcl/Path.swift\n",
      "Completed resolution in 1.92s\n",
      "Cloning https://github.com/mxcl/Path.swift\n",
      "Resolving https://github.com/mxcl/Path.swift at 0.16.2\n",
      "Compile Swift Module 'Path' (9 sources)\n",
      "Compile Swift Module 'jupyterInstalledPackages' (1 sources)\n",
      "Linking ./.build/x86_64-unknown-linux/debug/libjupyterInstalledPackages.so\n",
      "Initializing Swift...\n",
      "Loading library...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install '.package(url: \"https://github.com/mxcl/Path.swift\", from: \"0.16.1\")' Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Path\n",
    "import TensorFlow\n",
    "import Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## //TODO add plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "let np = Python.import(\"numpy\")\n",
    "let plt = Python.import(\"matplotlib.pyplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('inline', 'module://ipykernel.pylab.backend_inline')\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "func loadData<Scalar: TensorFlowNumeric>(\n",
    "    path: String, shape: [Int], is_label: Bool\n",
    ") -> Tensor<Scalar> {\n",
    "    let dropK: Int = (is_label ? 8 : 16)\n",
    "    let data = try! Data.init(contentsOf: \n",
    "                     URL.init(fileURLWithPath: path)\n",
    "                    ).dropFirst(dropK)\n",
    "    let tensorShape = TensorShape.init(shape)\n",
    "    let outTensor = Tensor<Float>(data.map(Float.init)).reshaped(to: tensorShape)\n",
    "    return Tensor<Scalar>(outTensor)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "let basepath = Path.home/\".fastai\"/\"data\"/\"mnist\"\n",
    "let trnImgs = \"train-images-idx3-ubyte\"\n",
    "let trnLbls = \"train-labels-idx1-ubyte\"\n",
    "let valImgs = \"t10k-images-idx3-ubyte\"\n",
    "let valLbls = \"t10k-labels-idx1-ubyte\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "let xTrain: Tensor<Float> = loadData(path: (basepath/trnImgs).string,\n",
    "                   shape: [60000, 784],\n",
    "                   is_label: false)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "let yTrain: Tensor<Int32> = loadData(path: (basepath/trnLbls).string,\n",
    "                   shape: [60000],\n",
    "                   is_label: true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "let xValid: Tensor<Float> = loadData(path: (basepath/valImgs).string,\n",
    "                   shape: [10000, 784],\n",
    "                   is_label: false)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "let yValid: Tensor<Int32> = loadData(path: (basepath/valLbls).string,\n",
    "                   shape: [10000],\n",
    "                   is_label: true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "public extension Tensor where Scalar : TensorFlowFloatingPoint {\n",
    "    func stddev(alongAxes axes: [Int])-> Tensor<Scalar>{\n",
    "        let mean = self.mean(alongAxes: axes)\n",
    "        return sqrt((self - mean).squared().mean(alongAxes: axes))\n",
    "    }\n",
    "    \n",
    "    func stddev()-> Tensor<Scalar>{\n",
    "        let mean = self.mean()\n",
    "        return sqrt((self - mean).squared().mean())\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "func normalize<Scalar: TensorFlowFloatingPoint>(\n",
    "    _ x: Tensor<Scalar>, _ mean: Tensor<Scalar>? = nil, _ stddev: Tensor<Scalar>? = nil\n",
    ") ->  Tensor<Scalar>{\n",
    "    var mean = (mean ?? x.mean())\n",
    "    var stddev = (stddev ?? x.stddev())\n",
    "    return (x-mean)/stddev\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "var xTrainNormal: Tensor<Float> = normalize(xTrain)\n",
    "var xValidNormal: Tensor<Float> = normalize(\n",
    "    xValid,\n",
    "    xTrain.mean(),\n",
    "    xTrain.stddev()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct Model<Scalar: TensorFlowFloatingPoint>: Layer {\n",
    "    public var linear1: Dense<Scalar>\n",
    "    public var linear2: Dense<Scalar>\n",
    "\n",
    "    public init(_ nIn: Int, _ nHidden: Int, _ nOut: Int){\n",
    "        self.linear1 = Dense(inputSize: nIn, outputSize: nHidden, activation: relu)\n",
    "        self.linear2 = Dense(inputSize: nHidden, outputSize: nOut)\n",
    "    }\n",
    "\n",
    "    @differentiable\n",
    "    public func call(_ input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        return input.sequenced(through: linear1, linear2)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "let n: Int = xTrainNormal.shape[0]\n",
    "let m: Int = xTrainNormal.shape[1]\n",
    "let c: Int = Int(yTrain.max().scalarized()) + 1\n",
    "let nh: Int = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 784 10 50\r\n"
     ]
    }
   ],
   "source": [
    "print(n,m,c,nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "var model = Model<Float>(m, nh, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "var preds = model(xTrainNormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy loss\n",
    "First, we will need to compute the softmax of our activations. This is defined by:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n",
    "\n",
    "or more concisely:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}}$$\n",
    "\n",
    "In practice, we will need the log of the softmax when we calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "func logSoftmax<Scalar: FloatingPoint>(_ x: Tensor<Scalar>) -> Tensor<Scalar>{\n",
    "    var eX = exp(x)\n",
    "    return log(eX/eX.sum(alongAxes: -1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "var smPreds = logSoftmax(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "‚ñø TensorShape\n",
       "  ‚ñø dimensions : 2 elements\n",
       "    - 0 : 60000\n",
       "    - 1 : 10\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smPreds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0504041\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smPreds[0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0504041, -3.5830793, -2.5765488]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Raw.gatherNd(params: smPreds, indices: Tensor<Int32>([[0, 5],[1, 0],[2, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "func nll<Scalar: TensorFlowFloatingPoint, TI: TensorFlowInteger>(\n",
    "    _ preds: Tensor<Scalar>, _ target: Tensor<TI>\n",
    ") -> Tensor<Scalar>{\n",
    "    let rng = Tensor<TI>(rangeFrom: TI(0), to: TI(preds.shape[0]), stride: TI(1))\n",
    "    let idx: Tensor<TI> = Raw.concat(concatDim: Tensor(1),\n",
    "                                [rng.expandingShape(at: -1), \n",
    "                                 Tensor<TI>(target.expandingShape(at: -1))])\n",
    "    return -Raw.gatherNd(params: preds, indices: idx).mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8407245\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll(smPreds, yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the formula:\n",
    "$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$\n",
    "gives a simplification when we compute the log softmax, which was previously defined as (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "func logSoftmax<Scalar: FloatingPoint>(_ x: Tensor<Scalar>) -> Tensor<Scalar>{\n",
    "    var eX = exp(x)\n",
    "    return x - log(eX.sum(alongAxes: -1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "var smPreds = logSoftmax(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8407245\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll(smPreds, yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp). The idea is to use the following formula:\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
    "where a is the maximum of the $x_{j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "func logSumExp<Scalar: FloatingPoint>(_ x: Tensor<Scalar>) -> Tensor<Scalar>{\n",
    "    var a = x.max(alongAxes: -1)\n",
    "    return a + log(exp(x-a).sum(alongAxes: -1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "func logSoftmax<Scalar: FloatingPoint>(_ x: Tensor<Scalar>) -> Tensor<Scalar>{\n",
    "    return x - logSumExp(x)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8407245\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll(logSoftmax(preds), yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now S4TF's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8407245\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmaxCrossEntropy(logits: preds, labels: yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic training loop\n",
    "\n",
    "Basically the training loop repeats over the following steps:\n",
    "\n",
    "- get the output of the model on a batch of inputs\n",
    "- compare the output to the labels we have and compute a loss\n",
    "- calculate the gradients of the loss with respect to every parameter of the model\n",
    "- update said parameters with those gradients to make them a little bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "func accuracy<TF: TensorFlowFloatingPoint>(\n",
    "    _ preds: Tensor<TF>, _ targets: Tensor<Int32>\n",
    ")-> Tensor<TF>{\n",
    "    return Tensor<TF>(preds.argmax(squeezingAxis: -1) .== targets).mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.094\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "var bs = 64\n",
    "var xb = xTrainNormal[0..<bs]\n",
    "var preds = model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09960985,   0.7754209,   1.1765766,   1.3046979,   1.1709158,   2.5706964,   1.6591058,\r\n",
      "   1.4501945,  0.12702267,   -0.871587] TensorShape(dimensions: [64, 10])\r\n"
     ]
    }
   ],
   "source": [
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9246294\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var yb = yTrain[0..<bs]\n",
    "softmaxCrossEntropy(logits: preds, labels: yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.078125\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "var lr: Float = 0.05\n",
    "var epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 0..<epochs {\n",
    "    for i in 0..<((n-1)/bs + 1){\n",
    "        var start_i = i * bs\n",
    "        var end_i = start_i + bs\n",
    "        var xb = xTrainNormal[start_i..<end_i]\n",
    "        var yb = yTrain[start_i..<end_i]\n",
    "        \n",
    "        var (preds, backprop) = model.appliedForBackpropagation(to: xb)\n",
    "        var (loss, grad) = preds.valueWithGradient{preds in\n",
    "                            softmaxCrossEntropy(logits: preds,  labels: yb)}\n",
    "        var (ùõÅmodel, _) = backprop(grad)\n",
    "        \n",
    "        //this is sort of like parameters in PyTorch\n",
    "        for kp in model.allDifferentiableVariables.recursivelyAllWritableKeyPaths(to: Tensor<Float>.self){\n",
    "            model.allDifferentiableVariables[keyPath: kp] -= ùõÅmodel[keyPath: kp] * lr\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.101875655 0.953125\r\n"
     ]
    }
   ],
   "source": [
    "var preds = model(xb)\n",
    "print(softmaxCrossEntropy(logits: preds,  labels: yb), accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "Let's replace our previous manually coded optimization step:\n",
    "\n",
    "```\n",
    "for kp in model.allDifferentiableVariables.recursivelyAllWritableKeyPaths(to: Tensor<Float>.self){\n",
    "    model.allDifferentiableVariables[keyPath: kp] -= ùõÅmodel[keyPath: kp] * lr\n",
    "}\n",
    "```\n",
    "with:\n",
    "```\n",
    "opt.step(&model, ùõÅmodel)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct MyOptimizer<Model: Layer> {\n",
    "    public var lr: Float\n",
    "\n",
    "    public init(_ lr: Float=0.05){\n",
    "        self.lr = lr\n",
    "    }\n",
    "    \n",
    "    mutating public func step(\n",
    "        _ model: inout Model, _ grad: Model.AllDifferentiableVariables\n",
    "    ) {\n",
    "        for kp in model.allDifferentiableVariables.recursivelyAllWritableKeyPaths(to: Tensor<Float>.self){\n",
    "            model.allDifferentiableVariables[keyPath: kp] -= grad[keyPath: kp] * self.lr\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "var model = Model<Float>(m, nh, c)\n",
    "var opt = MyOptimizer<Model<Float>>()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 0..<epochs {\n",
    "    for i in 0..<((n-1)/bs + 1){\n",
    "        var start_i = i * bs\n",
    "        var end_i = start_i + bs\n",
    "        var xb = xTrainNormal[start_i..<end_i]\n",
    "        var yb = yTrain[start_i..<end_i]\n",
    "        \n",
    "        var (preds, backprop) = model.appliedForBackpropagation(to: xb)\n",
    "        var (loss, grad) = preds.valueWithGradient{preds in\n",
    "                            softmaxCrossEntropy(logits: preds,  labels: yb)}\n",
    "        var (ùõÅmodel, _) = backprop(grad)\n",
    "\n",
    "        opt.step(&model, ùõÅmodel)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16002208 0.953125\r\n"
     ]
    }
   ],
   "source": [
    "var preds = model(xb)\n",
    "print(softmaxCrossEntropy(logits: preds,  labels: yb), accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S4TF already provides similar functionality in ```SGD```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "var model = Model<Float>(m, nh, c)\n",
    "var opt = SGD(for: model, learningRate: lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 0..<epochs {\n",
    "    for i in 0..<((n-1)/bs + 1){\n",
    "        var start_i = i * bs\n",
    "        var end_i = start_i + bs\n",
    "        var xb = xTrainNormal[start_i..<end_i]\n",
    "        var yb = yTrain[start_i..<end_i]\n",
    "        \n",
    "        var (preds, backprop) = model.appliedForBackpropagation(to: xb)\n",
    "        var (loss, grad) = preds.valueWithGradient{preds in\n",
    "                            softmaxCrossEntropy(logits: preds,  labels: yb)}\n",
    "        var (ùõÅmodel, _) = backprop(grad)\n",
    "\n",
    "        opt.update(&model.allDifferentiableVariables, along: ùõÅmodel)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11824095 0.96875\r\n"
     ]
    }
   ],
   "source": [
    "var preds = model(xb)\n",
    "print(softmaxCrossEntropy(logits: preds,  labels: yb), accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, our loop iterated over batches (xb, yb) like this:\n",
    "```\n",
    "    for i in 0..<((n-1)/bs + 1){\n",
    "        var start_i = i * bs\n",
    "        var end_i = start_i + bs\n",
    "        var xb = xTrainNormal[start_i..<end_i]\n",
    "        var yb = yTrain[start_i..<end_i] \n",
    "        ...\n",
    "    }\n",
    "```\n",
    "Let's make our loop much cleaner, using a DataSet:\n",
    "```\n",
    "    for batch in trainDataset.batched(bs){\n",
    "        batch.inputs\n",
    "        batch.labels\n",
    "        ...\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct Batch<Inputs: Differentiable & TensorGroup, Labels: TensorGroup>: TensorGroup {\n",
    "    public var inputs: Inputs\n",
    "    public var labels: Labels\n",
    "    \n",
    "    init(_ inputs: Inputs, _ labels: Labels){\n",
    "        (self.inputs, self.labels) = (inputs, labels)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "var trainDataset = Dataset<Batch>(elements: Batch(xTrainNormal, yTrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "var it = trainDataset.makeIterator()\n",
    "var img = np.asarray(it.next()!.inputs.array.scalars).reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKAa3AAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize: [5,5])\n",
    "let fig = plt.imshow(X:img, cmap:\"gray\")\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "var model = Model<Float>(m, nh, c)\n",
    "var opt = SGD(for: model, learningRate: lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "var bs = 64\n",
    "var lr: Float = 0.05\n",
    "var epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 0..<epochs {\n",
    "    for batch in trainDataset.batched(bs){\n",
    "        var (preds, backprop) = model.appliedForBackpropagation(to: batch.inputs)\n",
    "        var (loss, grad) = preds.valueWithGradient{preds in\n",
    "                            softmaxCrossEntropy(logits: preds,  labels: batch.labels)}\n",
    "        var (ùõÅmodel, _) = backprop(grad)\n",
    "\n",
    "        opt.update(&model.allDifferentiableVariables, along: ùõÅmodel)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13320169 0.96875\r\n"
     ]
    }
   ],
   "source": [
    "var it = trainDataset.batched(bs).makeIterator()\n",
    "var batch = it.next()!\n",
    "var preds = model(batch.inputs)\n",
    "print(softmaxCrossEntropy(logits: preds,  labels: batch.labels), accuracy(preds, batch.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with random shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 0..<epochs {\n",
    "    for batch in trainDataset.batched(bs).shuffled(sampleCount: n, randomSeed: 3){\n",
    "        var (preds, backprop) = model.appliedForBackpropagation(to: batch.inputs)\n",
    "        var (loss, grad) = preds.valueWithGradient{preds in\n",
    "                            softmaxCrossEntropy(logits: preds,  labels: batch.labels)}\n",
    "        var (ùõÅmodel, _) = backprop(grad)\n",
    "\n",
    "        opt.update(&model.allDifferentiableVariables, along: ùõÅmodel)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1273525 0.96875\r\n"
     ]
    }
   ],
   "source": [
    "var batch = it.next()!\n",
    "var preds = model(batch.inputs)\n",
    "print(softmaxCrossEntropy(logits: preds,  labels: batch.labels), accuracy(preds, batch.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "You **always** should also have a [validation set](http://www.fast.ai/2017/11/13/validation-sets/), in order to identify if you are overfitting.\n",
    "\n",
    "We will calculate and print the validation loss at the end of each epoch.\n",
    "\n",
    "(Note that we always call model.train() before training, and model.eval() before inference, because these are used by layers such as nn.BatchNorm2d and nn.Dropout to ensure appropriate behaviour for these different phases.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "func fit<Opt: Optimizer, Labels: TensorGroup>(\n",
    "    epochs: Int,\n",
    "    model: inout Opt.Model,\n",
    "    opt: inout Opt,\n",
    "    trainDs: Dataset<Batch<Opt.Model.Input, Labels>>,\n",
    "    validDs: Dataset<Batch<Opt.Model.Input, Labels>>,\n",
    "    lossFunc: @escaping @differentiable (_ logits: Tensor<Opt.Scalar>, _ labels: @nondiff Labels) -> Tensor<Opt.Scalar>\n",
    ") where\n",
    "    Opt.Model: Layer,\n",
    "    Opt.Model.Input: TensorGroup,\n",
    "    Opt.Scalar: TensorFlowFloatingPoint,\n",
    "    Opt.Model.Output == Tensor<Opt.Scalar>\n",
    "{\n",
    "    for epoch in 0..<epochs {\n",
    "        for batch in trainDs{\n",
    "            var (_, ùõÅmodel) = model.valueWithGradient{\n",
    "                model -> Tensor<Opt.Scalar> in\n",
    "                    let preds = model(batch.inputs)\n",
    "                    return lossFunc(preds,  batch.labels)}\n",
    "            opt.update(&model.allDifferentiableVariables, along: ùõÅmodel)\n",
    "        }\n",
    "\n",
    "        var totalLoss = Tensor<Opt.Scalar>(0)\n",
    "        var totalAccuracy = Tensor<Opt.Scalar>(0)\n",
    "        var it = Opt.Scalar(0)\n",
    "        for batch in validDs{\n",
    "            let preds = model(batch.inputs)\n",
    "            let loss = softmaxCrossEntropy(logits: preds, labels: batch.labels as! Tensor<Int32>)\n",
    "            let acc = accuracy(preds, batch.labels as! Tensor<Int32>)\n",
    "            totalLoss += loss\n",
    "            totalAccuracy += acc\n",
    "            it += Opt.Scalar(1)\n",
    "        }\n",
    "        print(epoch, totalLoss/it, totalAccuracy/it)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "var trainDataset = Dataset<Batch>(\n",
    "    elements: Batch(xTrainNormal, yTrain)\n",
    ").shuffled(sampleCount: n, randomSeed: 3).batched(bs)\n",
    "var validDataset = Dataset<Batch>(\n",
    "    elements: Batch(xValidNormal, yValid)\n",
    ").batched(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "var bs = 64\n",
    "var lr: Float = 0.05\n",
    "var epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "var model = Model<Float>(m, nh, c)\n",
    "var opt = SGD(for: model, learningRate: lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.17526488 0.94994026\n",
      "1 0.13379017 0.95899683\n",
      "2 0.11517216 0.96536624\n",
      "3 0.10422918 0.96924764\n"
     ]
    }
   ],
   "source": [
    "fit(epochs: 4, \n",
    "    model: &model,\n",
    "    opt: &opt,\n",
    "    trainDs: trainDataset,\n",
    "    validDs: validDataset,\n",
    "    lossFunc: softmaxCrossEntropy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
