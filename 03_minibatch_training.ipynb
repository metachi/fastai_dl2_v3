{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a reimplementation of fastai part 2 version 3 in Swift. https://github.com/fastai/fastai_docs/blob/master/dev_course/dl2/03_minibatch_training.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(url: \"https://github.com/mxcl/Path.swift\", from: \"0.16.1\")\n",
      "\t\tPath\n",
      "With SwiftPM flags: []\n",
      "Working in: /tmp/tmp9th6dc85\n",
      "Fetching https://github.com/mxcl/Path.swift\n",
      "Completed resolution in 0.94s\n",
      "Cloning https://github.com/mxcl/Path.swift\n",
      "Resolving https://github.com/mxcl/Path.swift at 0.16.2\n",
      "Compile Swift Module 'Path' (9 sources)\n",
      "Compile Swift Module 'jupyterInstalledPackages' (1 sources)\n",
      "Linking ./.build/x86_64-unknown-linux/debug/libjupyterInstalledPackages.so\n",
      "Initializing Swift...\n",
      "Loading library...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install '.package(url: \"https://github.com/mxcl/Path.swift\", from: \"0.16.1\")' Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Path\n",
    "import TensorFlow\n",
    "import Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## //TODO add plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// let np = Python.import(\"numpy\")\n",
    "// let plt = Python.import(\"matplotlib.pyplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "// %include \"EnableIPythonDisplay.swift\"\n",
    "// IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "func loadData<Scalar: TensorFlowNumeric>(\n",
    "    path: String, shape: [Int], is_label: Bool\n",
    ") -> Tensor<Scalar> {\n",
    "    let dropK: Int = (is_label ? 8 : 16)\n",
    "    let data = try! Data.init(contentsOf: \n",
    "                     URL.init(fileURLWithPath: path)\n",
    "                    ).dropFirst(dropK)\n",
    "    let tensorShape = TensorShape.init(shape)\n",
    "    let outTensor = Tensor<Float>(data.map(Float.init)).reshaped(to: tensorShape)\n",
    "    return Tensor<Scalar>(outTensor)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "let basepath = Path.home/\".fastai\"/\"data\"/\"mnist\"\n",
    "let trnImgs = \"train-images-idx3-ubyte\"\n",
    "let trnLbls = \"train-labels-idx1-ubyte\"\n",
    "let valImgs = \"t10k-images-idx3-ubyte\"\n",
    "let valLbls = \"t10k-labels-idx1-ubyte\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "let xTrain: Tensor<Float> = loadData(path: (basepath/trnImgs).string,\n",
    "                   shape: [60000, 784],\n",
    "                   is_label: false)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "let yTrain: Tensor<Int32> = loadData(path: (basepath/trnLbls).string,\n",
    "                   shape: [60000],\n",
    "                   is_label: true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "let xValid: Tensor<Float> = loadData(path: (basepath/valImgs).string,\n",
    "                   shape: [10000, 784],\n",
    "                   is_label: false)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "let yValid: Tensor<Int32> = loadData(path: (basepath/valLbls).string,\n",
    "                   shape: [10000],\n",
    "                   is_label: true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "public extension Tensor where Scalar : TensorFlowFloatingPoint {\n",
    "    func stddev(alongAxes axes: [Int])-> Tensor<Scalar>{\n",
    "        let mean = self.mean(alongAxes: axes)\n",
    "        return sqrt((self - mean).squared().mean(alongAxes: axes))\n",
    "    }\n",
    "    \n",
    "    func stddev()-> Tensor<Scalar>{\n",
    "        let mean = self.mean()\n",
    "        return sqrt((self - mean).squared().mean())\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "func normalize<Scalar: TensorFlowFloatingPoint>(\n",
    "    _ x: Tensor<Scalar>, _ mean: Tensor<Scalar>? = nil, _ stddev: Tensor<Scalar>? = nil\n",
    ") ->  Tensor<Scalar>{\n",
    "    var mean = (mean ?? x.mean())\n",
    "    var stddev = (stddev ?? x.stddev())\n",
    "    return (x-mean)/stddev\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "var xTrainNormal: Tensor<Float> = normalize(xTrain)\n",
    "var xValidNormal: Tensor<Float> = normalize(\n",
    "    xValid,\n",
    "    xTrain.mean(),\n",
    "    xTrain.stddev()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct Model<Scalar: TensorFlowFloatingPoint>: Layer {\n",
    "    public var linear1: Dense<Scalar>\n",
    "    public var linear2: Dense<Scalar>\n",
    "\n",
    "    public init(_ nIn: Int, _ nHidden: Int, _ nOut: Int){\n",
    "        self.linear1 = Dense(inputSize: nIn, outputSize: nHidden, activation: relu)\n",
    "        self.linear2 = Dense(inputSize: nHidden, outputSize: nOut)\n",
    "    }\n",
    "\n",
    "    @differentiable\n",
    "    public func call(_ input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        return input.sequenced(through: linear1, linear2)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "let n: Int = xTrainNormal.shape[0]\n",
    "let m: Int = xTrainNormal.shape[1]\n",
    "let c: Int = Int(yTrain.max().scalarized()) + 1\n",
    "let nh: Int = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 784 10 50\r\n"
     ]
    }
   ],
   "source": [
    "print(n,m,c,nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "var model = Model<Float>(m, nh, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "var preds = model(xTrainNormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy loss\n",
    "First, we will need to compute the softmax of our activations. This is defined by:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n",
    "\n",
    "or more concisely:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}}$$\n",
    "\n",
    "In practice, we will need the log of the softmax when we calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "func logSoftmax<Scalar: FloatingPoint>(_ x: Tensor<Scalar>) -> Tensor<Scalar>{\n",
    "    var eX = exp(x)\n",
    "    return log(eX/eX.sum(alongAxes: -1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "var smPreds = logSoftmax(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ TensorShape\n",
       "  ▿ dimensions : 2 elements\n",
       "    - 0 : 60000\n",
       "    - 1 : 10\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smPreds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.309116\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smPreds[0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ -2.309116, -2.9300528,  -2.891936]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Raw.gatherNd(params: smPreds, indices: Tensor<Int32>([[0, 5],[1, 0],[2, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "func nll<Scalar: TensorFlowFloatingPoint, TI: TensorFlowInteger>(\n",
    "    _ preds: Tensor<Scalar>, _ target: Tensor<TI>\n",
    ") -> Tensor<Scalar>{\n",
    "    let rng = Tensor<TI>(rangeFrom: TI(0), to: TI(preds.shape[0]), stride: TI(1))\n",
    "    let idx: Tensor<TI> = Raw.concat(concatDim: Tensor(1),\n",
    "                                [rng.expandingShape(at: -1), \n",
    "                                 Tensor<TI>(target.expandingShape(at: -1))])\n",
    "    return -Raw.gatherNd(params: preds, indices: idx).mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7659333\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll(smPreds, yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the formula:\n",
    "$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$\n",
    "gives a simplification when we compute the log softmax, which was previously defined as (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "func logSoftmax<Scalar: FloatingPoint>(_ x: Tensor<Scalar>) -> Tensor<Scalar>{\n",
    "    var eX = exp(x)\n",
    "    return x - log(eX.sum(alongAxes: -1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "var smPreds = logSoftmax(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7659333\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll(smPreds, yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp). The idea is to use the following formula:\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
    "where a is the maximum of the $x_{j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "func logSumExp<Scalar: FloatingPoint>(_ x: Tensor<Scalar>) -> Tensor<Scalar>{\n",
    "    var a = x.max(alongAxes: -1)\n",
    "    return a + log(exp(x-a).sum(alongAxes: -1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "func logSoftmax<Scalar: FloatingPoint>(_ x: Tensor<Scalar>) -> Tensor<Scalar>{\n",
    "    return x - logSumExp(x)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7659333\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll(logSoftmax(preds), yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now S4TF's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7659333\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmaxCrossEntropy(logits: preds, labels: yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic training loop\n",
    "\n",
    "Basically the training loop repeats over the following steps:\n",
    "\n",
    "- get the output of the model on a batch of inputs\n",
    "- compare the output to the labels we have and compute a loss\n",
    "- calculate the gradients of the loss with respect to every parameter of the model\n",
    "- update said parameters with those gradients to make them a little bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "func accuracy<TF: TensorFlowFloatingPoint>(\n",
    "    _ preds: Tensor<TF>, _ targets: Tensor<Int32>\n",
    ")-> Tensor<TF>{\n",
    "    return Tensor<TF>(preds.argmax(squeezingAxis: -1) .== targets).mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1215\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "var bs = 64\n",
    "var xb = xTrainNormal[0..<bs]\n",
    "var preds = model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.2074938,  -0.7015091,   1.3807161,  -1.4192412,   -0.380169,  0.44342202,   0.8917738,\r\n",
      " -0.41923478,   1.6071327, -0.63552344] TensorShape(dimensions: [64, 10])\r\n"
     ]
    }
   ],
   "source": [
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9021833\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var yb = yTrain[0..<bs]\n",
    "softmaxCrossEntropy(logits: preds, labels: yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "var lr: Float = 0.05\n",
    "var epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 0..<epochs {\n",
    "    for i in 0..<((n-1)/bs + 1){\n",
    "        var start_i = i * bs\n",
    "        var end_i = start_i + bs\n",
    "        var xb = xTrainNormal[start_i..<end_i]\n",
    "        var yb = yTrain[start_i..<end_i]\n",
    "        \n",
    "        var (preds, backprop) = model.appliedForBackpropagation(to: xb)\n",
    "        var (loss, grad) = preds.valueWithGradient{preds in\n",
    "                            softmaxCrossEntropy(logits: preds,  labels: yb)}\n",
    "        var (𝛁model, _) = backprop(grad)\n",
    "        \n",
    "        //this is sort of like parameters in PyTorch\n",
    "        for kp in model.allDifferentiableVariables.recursivelyAllWritableKeyPaths(to: Tensor<Float>.self){\n",
    "            model.allDifferentiableVariables[keyPath: kp] -= 𝛁model[keyPath: kp] * lr\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09727964 0.96875\r\n"
     ]
    }
   ],
   "source": [
    "var preds = model(xb)\n",
    "print(softmaxCrossEntropy(logits: preds,  labels: yb), accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "Let's replace our previous manually coded optimization step:\n",
    "\n",
    "```\n",
    "for kp in model.allDifferentiableVariables.recursivelyAllWritableKeyPaths(to: Tensor<Float>.self){\n",
    "    model.allDifferentiableVariables[keyPath: kp] -= 𝛁model[keyPath: kp] * lr\n",
    "}\n",
    "```\n",
    "with:\n",
    "```\n",
    "opt.step(&model, 𝛁model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct MyOptimizer<Model: Layer> {\n",
    "    public var lr: Float\n",
    "\n",
    "    public init(_ lr: Float=0.05){\n",
    "        self.lr = lr\n",
    "    }\n",
    "    \n",
    "    mutating public func step(\n",
    "        _ model: inout Model, _ grad: Model.AllDifferentiableVariables\n",
    "    ) {\n",
    "        for kp in model.allDifferentiableVariables.recursivelyAllWritableKeyPaths(to: Tensor<Float>.self){\n",
    "            model.allDifferentiableVariables[keyPath: kp] -= grad[keyPath: kp] * self.lr\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "var model = Model<Float>(m, nh, c)\n",
    "var opt = MyOptimizer<Model<Float>>()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 0..<epochs {\n",
    "    for i in 0..<((n-1)/bs + 1){\n",
    "        var start_i = i * bs\n",
    "        var end_i = start_i + bs\n",
    "        var xb = xTrainNormal[start_i..<end_i]\n",
    "        var yb = yTrain[start_i..<end_i]\n",
    "        \n",
    "        var (preds, backprop) = model.appliedForBackpropagation(to: xb)\n",
    "        var (loss, grad) = preds.valueWithGradient{preds in\n",
    "                            softmaxCrossEntropy(logits: preds,  labels: yb)}\n",
    "        var (𝛁model, _) = backprop(grad)\n",
    "\n",
    "        opt.step(&model, 𝛁model)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12687418 0.96875\r\n"
     ]
    }
   ],
   "source": [
    "var preds = model(xb)\n",
    "print(softmaxCrossEntropy(logits: preds,  labels: yb), accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S4TF already provides similar functionality in ```SGD```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "var model = Model<Float>(m, nh, c)\n",
    "var opt = SGD(for: model, learningRate: lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 0..<epochs {\n",
    "    for i in 0..<((n-1)/bs + 1){\n",
    "        var start_i = i * bs\n",
    "        var end_i = start_i + bs\n",
    "        var xb = xTrainNormal[start_i..<end_i]\n",
    "        var yb = yTrain[start_i..<end_i]\n",
    "        \n",
    "        var (preds, backprop) = model.appliedForBackpropagation(to: xb)\n",
    "        var (loss, grad) = preds.valueWithGradient{preds in\n",
    "                            softmaxCrossEntropy(logits: preds,  labels: yb)}\n",
    "        var (𝛁model, _) = backprop(grad)\n",
    "\n",
    "        opt.update(&model.allDifferentiableVariables, along: 𝛁model)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1339483 0.96875\r\n"
     ]
    }
   ],
   "source": [
    "var preds = model(xb)\n",
    "print(softmaxCrossEntropy(logits: preds,  labels: yb), accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, our loop iterated over batches (xb, yb) like this:\n",
    "```\n",
    "    for i in 0..<((n-1)/bs + 1){\n",
    "        var start_i = i * bs\n",
    "        var end_i = start_i + bs\n",
    "        var xb = xTrainNormal[start_i..<end_i]\n",
    "        var yb = yTrain[start_i..<end_i] \n",
    "        ...\n",
    "    }\n",
    "```\n",
    "Let's make our loop much cleaner, using a DataSet:\n",
    "```\n",
    "    for batch in trainDataset.batched(bs){\n",
    "        batch.inputs\n",
    "        batch.labels\n",
    "        ...\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct Batch<Inputs: Differentiable & TensorGroup, Labels: TensorGroup>: TensorGroup {\n",
    "    public var inputs: Inputs\n",
    "    public var labels: Labels\n",
    "    \n",
    "    init(_ inputs: Inputs, _ labels: Labels){\n",
    "        (self.inputs, self.labels) = (inputs, labels)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "var trainDataset = Dataset<Batch>(elements: Batch(xTrainNormal, yTrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "// var it = trainDataset.makeIterator()\n",
    "// var img = np.asarray(it.next()!.inputs.array.scalars).reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "// plt.figure(figsize: [5,5])\n",
    "// let fig = plt.imshow(X:img, cmap:\"gray\")\n",
    "// plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "var model = Model<Float>(m, nh, c)\n",
    "var opt = SGD(for: model, learningRate: lr) //note: this bombs w/o the scalarType arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "var bs = 64\n",
    "var lr: Float = 0.05\n",
    "var epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 0..<epochs {\n",
    "    for batch in trainDataset.batched(bs){\n",
    "        var (preds, backprop) = model.appliedForBackpropagation(to: batch.inputs)\n",
    "        var (loss, grad) = preds.valueWithGradient{preds in\n",
    "                            softmaxCrossEntropy(logits: preds,  labels: batch.labels)}\n",
    "        var (𝛁model, _) = backprop(grad)\n",
    "\n",
    "        opt.update(&model.allDifferentiableVariables, along: 𝛁model)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13766007 0.984375\r\n"
     ]
    }
   ],
   "source": [
    "var it = trainDataset.batched(bs).makeIterator()\n",
    "var batch = it.next()!\n",
    "var preds = model(batch.inputs)\n",
    "print(softmaxCrossEntropy(logits: preds,  labels: batch.labels), accuracy(preds, batch.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with random shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 0..<epochs {\n",
    "    for batch in trainDataset.batched(bs).shuffled(sampleCount: n, randomSeed: 3){\n",
    "        var (preds, backprop) = model.appliedForBackpropagation(to: batch.inputs)\n",
    "        var (loss, grad) = preds.valueWithGradient{preds in\n",
    "                            softmaxCrossEntropy(logits: preds,  labels: batch.labels)}\n",
    "        var (𝛁model, _) = backprop(grad)\n",
    "\n",
    "        opt.update(&model.allDifferentiableVariables, along: 𝛁model)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.108013384 0.984375\r\n"
     ]
    }
   ],
   "source": [
    "var batch = it.next()!\n",
    "var preds = model(batch.inputs)\n",
    "print(softmaxCrossEntropy(logits: preds,  labels: batch.labels), accuracy(preds, batch.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "You **always** should also have a [validation set](http://www.fast.ai/2017/11/13/validation-sets/), in order to identify if you are overfitting.\n",
    "\n",
    "We will calculate and print the validation loss at the end of each epoch.\n",
    "\n",
    "(Note that we always call model.train() before training, and model.eval() before inference, because these are used by layers such as nn.BatchNorm2d and nn.Dropout to ensure appropriate behaviour for these different phases.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: confirm grads are not being accumulated in validation step after change to context struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "func fit<Opt: Optimizer, Labels: TensorGroup>(\n",
    "    epochs: Int,\n",
    "    model: inout Opt.Model,\n",
    "    opt: inout Opt,\n",
    "    trainDs: Dataset<Batch<Opt.Model.Input, Labels>>,\n",
    "    validDs: Dataset<Batch<Opt.Model.Input, Labels>>,\n",
    "    lossFunc: @escaping @differentiable (_ logits: Tensor<Opt.Scalar>, _ labels: @nondiff Labels) -> Tensor<Opt.Scalar>\n",
    ") where\n",
    "    Opt.Model: Layer,\n",
    "    Opt.Model.Input: TensorGroup,\n",
    "    Opt.Scalar: TensorFlowFloatingPoint,\n",
    "    Opt.Model.Output == Tensor<Opt.Scalar>\n",
    "{\n",
    "    for epoch in 0..<epochs {\n",
    "        for batch in trainDs{\n",
    "            var (_, 𝛁model) = model.valueWithGradient{\n",
    "                model -> Tensor<Opt.Scalar> in\n",
    "                    let preds = model(batch.inputs)\n",
    "                    return lossFunc(preds,  batch.labels)}\n",
    "            opt.update(&model.allDifferentiableVariables, along: 𝛁model)\n",
    "        }\n",
    "\n",
    "        var totalLoss = Tensor<Opt.Scalar>(0)\n",
    "        var totalAccuracy = Tensor<Opt.Scalar>(0)\n",
    "        var it = Opt.Scalar(0)\n",
    "        for batch in validDs{\n",
    "            let preds = model(batch.inputs)\n",
    "            let loss = softmaxCrossEntropy(logits: preds, labels: batch.labels as! Tensor<Int32>)\n",
    "            let acc = accuracy(preds, batch.labels as! Tensor<Int32>)\n",
    "            totalLoss += loss\n",
    "            totalAccuracy += acc\n",
    "            it += Opt.Scalar(1)\n",
    "        }\n",
    "        print(epoch, totalLoss/it, totalAccuracy/it)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "var trainDataset = Dataset<Batch>(\n",
    "    elements: Batch(xTrainNormal, yTrain)\n",
    ").shuffled(sampleCount: n, randomSeed: 3).batched(bs)\n",
    "var validDataset = Dataset<Batch>(\n",
    "    elements: Batch(xValidNormal, yValid)\n",
    ").batched(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "var bs = 64\n",
    "var lr: Float = 0.05\n",
    "var epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "var model = Model<Float>(m, nh, c)\n",
    "var opt = SGD(for: model, learningRate: lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.19221282 0.9442675\n",
      "1 0.14929047 0.95551354\n",
      "2 0.1306739 0.962082\n",
      "3 0.118585415 0.96556526\n"
     ]
    }
   ],
   "source": [
    "fit(epochs: 4, \n",
    "    model: &model,\n",
    "    opt: &opt,\n",
    "    trainDs: trainDataset,\n",
    "    validDs: validDataset,\n",
    "    lossFunc: softmaxCrossEntropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
